{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"res/itm_logo.jpg\" width=\"300px\">\n",
    "\n",
    "## Inteligencia Artificial - IAI84\n",
    "### Instituto Tecnológico Metropolitano\n",
    "#### Pedro Atencio Ortiz - 2017\n",
    "\n",
    "\n",
    "En este notebook se aborda el tema de aprendizaje de máquina para clasificación binaria utilizando Regresión Logística:\n",
    "1. Propagación hacia adelante (forward propagation)\n",
    "2. Función de pérdida\n",
    "3. Función de costo\n",
    "4. Descenso del gradiente\n",
    "5. Predicción\n",
    "6. Regresión logística sobre un dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='res/logistic_regression/graph.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La regresión logística se puede entender como una regresión lineal, acompañada de una función logística (sigmoide) que permite determinar la clase a la que pertenece un objeto.\n",
    "- Solo se aplica en problemas de clasificación binaria lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 1. Propagación hacia adelante (backward propagation)\n",
    "<hr>\n",
    "## 1.1. Activación lineal (linear activation)\n",
    "\n",
    "A continuación implementemos la activación lineal definida como:\n",
    "\n",
    "## <center>$z = W^{T}X+b$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    " Use numpy implementation of dot product np.dot(A,B) to multiply W.T and b\n",
    "'''\n",
    "def linear_activation(W, b, X):\n",
    "    z = None\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2 #to be able to verify your result\n",
    "np.random.seed(seed)\n",
    "W = np.random.randn(2,1)\n",
    "b = np.random.rand()\n",
    "X = np.random.randn(2, 3)\n",
    "\n",
    "print(linear_activation(W,b,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado esperado: __[[ 1.39865165  1.29546477  0.94717078]]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 1.2. Activación logística (sigmoid activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación implementemos la función sigmoide definida como:\n",
    "\n",
    "## <center>$sig(z)=\\sigma(z) = \\frac{1}{1+e^{-z}}$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Use numpy implementation for exponential e^k -- np.exp(k)\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Returns sigmoid activation for array z\n",
    "    \n",
    "    Arguments:\n",
    "        z -- array of float values.\n",
    "    Returns:\n",
    "        a -- sigmoid activation.\n",
    "    '''\n",
    "    a = None \n",
    "    \n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "z = np.random.randn(1,3)\n",
    "print(sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valor esperado = __[[ 0.39729283  0.485937    0.10562821]]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 2. Función de perdida\n",
    "\n",
    "A continuación implementemos la función de perdida como:\n",
    "\n",
    "## <center>$loss(y, a)=L(y, a) = -(y.log(a)+(1-y).log(1-a))$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Use np.log(a) to implement natural logarithm.\n",
    "'''\n",
    "def loss(y, a):\n",
    "    '''\n",
    "    Logistic loss implementation.\n",
    "    \n",
    "    Arguments:\n",
    "        y -- original labels.\n",
    "        a -- predicted labels from forward propagation.\n",
    "    '''\n",
    "    logloss = None\n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2 #to be able to verify your result\n",
    "np.random.seed(seed)\n",
    "W = np.random.randn(2,1)\n",
    "b = np.random.rand()\n",
    "X = np.random.randn(2, 3)\n",
    "\n",
    "Y = np.array([[1,1,0]]) #original labels for features X\n",
    "A = sigmoid(linear_activation(W,b,X)) #forward activation\n",
    "\n",
    "print(\"Perdida dato a dato: \", loss(Y, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado esperado: __[[ 0.22068428,  0.24198147,  1.27491702]]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 3. Función de costo\n",
    "\n",
    "Implementos la función de costo definida como:\n",
    "\n",
    "## <center>$cost(logloss) = J(logloss) = \\frac{1}{m}\\sum^{i=1}_{m}logloss^{(i)}$</center>\n",
    "\n",
    "para $m$ ejemplos en el dataset $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Cost is defined as the mean of logistic loss.\n",
    "'''\n",
    "def cost(logloss):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss = np.array([[0.22068428,  0.24198147,  1.27491702]])\n",
    "print(\"costo: \", cost(logloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado esperado: __0.57919425666666668__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 4. Descenso del gradiente (Gradient Descent)\n",
    "\n",
    "El descenso del gradiente implica aplicar la regla de la cadena sobre el grafo de cómputo de la regresión logística y calcular las derivadas parciales de los parámetros $W$ y $b$ respecto a la función de pérdida. Posteriormente el negativo de la derivada de los parámetros se utiliza para actualizar los mismos.\n",
    "\n",
    "<img src='res/logistic_regression/graph_backward.png' width=700>\n",
    "\n",
    "<br>\n",
    "El algoritmo vectorizado del descenso del gradiente para la regresión logística es:\n",
    "\n",
    "<i>\n",
    "__FOR__ i=1->epochs:\n",
    "\n",
    "    //forward propagation\n",
    "    Z = W.T*X+b\n",
    "    A = sig(Z)\n",
    "    \n",
    "    //cálculo de los gradientes de los parámetros\n",
    "    dz = A-Y\n",
    "    dW = sum(X*dz) / m\n",
    "    db = sum(dz) / m\n",
    "    \n",
    "    //cálculo del costo\n",
    "    J = cost(loss(Y,A))\n",
    "    \n",
    "    //actualizacion de parametros\n",
    "    W = W - learning_rate*dW\n",
    "    b = b - learning_rate*db\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('m: ', 2)\n",
      "('W inicial: ', array([[ 0.1 ],\n",
      "       [-0.1 ],\n",
      "       [ 0.01]]))\n",
      "('b inicial: ', 0.1)\n"
     ]
    }
   ],
   "source": [
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "\n",
    "X = np.random.rand(3,2)\n",
    "Y = np.array([[0, 1]])\n",
    "\n",
    "m = X.shape[1]\n",
    "\n",
    "W = np.array([[0.1], [-0.1], [0.01]])\n",
    "b = 0.1\n",
    "\n",
    "print(\"m: \", m)\n",
    "print(\"W inicial: \",W)\n",
    "print(\"b inicial: \",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilicemos las funciones previamente diseñadas para completar el siguiente código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "for i in range(1000): #1000 iteraciones del descenso del gradiente\n",
    "    Z = None\n",
    "    A = None\n",
    "    \n",
    "    dz = None\n",
    "    dW = None\n",
    "    db = None\n",
    "    \n",
    "    J = None\n",
    "\n",
    "    W -= None\n",
    "    b -= None\n",
    "    \n",
    "    if(i%100 == 0):\n",
    "        print(\"costo: \", J)\n",
    "\n",
    "print(\"W actualizado: \",W)\n",
    "print(\"b actualizado: \",b)\n",
    "print(\"costo total: \", J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resultado esperado__: \n",
    "\n",
    "('costo: ', 0.35961193473699637)\n",
    "\n",
    "('costo: ', 0.34112286128068314)\n",
    "\n",
    "('costo: ', 0.32416576026635818)\n",
    "\n",
    "('costo: ', 0.3085804969161185)\n",
    "\n",
    "('costo: ', 0.29422579454831238)\n",
    "\n",
    "('costo: ', 0.28097695742118511)\n",
    "\n",
    "('costo: ', 0.26872383438602254)\n",
    "\n",
    "('costo: ', 0.25736901432660603)\n",
    "\n",
    "('costo: ', 0.24682623751141713)\n",
    "\n",
    "('costo: ', 0.23701900363846592)\n",
    "\n",
    "('W actualizado: ', array([[-4.43431392],\n",
    "       [-4.63431392],\n",
    "       [-4.52431392]]))\n",
    "       \n",
    "('b actualizado: ', 4.8105808996169204)\n",
    "\n",
    "('costo total: ', 0.22796765247707698)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 5. Predicción\n",
    "\n",
    "La predicción consiste en aplicar forward propagation utilizando los W y b optimizados mediante descenso del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(W,b,X):\n",
    "    z = linear_activation(W,b,X)\n",
    "    A = sigmoid(z)\n",
    "    return np.round(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 6. Regresión Logística sobre un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import generate_data, visualize, plot_decision_boundary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.Generemos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data('blobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color= ['red' if y == 1 else 'green' for y in np.squeeze(Y)]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = Y.reshape(1,len(Y))# reshape Y to satisfy shape (m, 1)\n",
    "X = X.T #transpose X to satisfy shape (nx, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Inicialicemos los parámetros W y b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. inicilicemos parametros W y b\n",
    "m = X.shape[1]\n",
    "\n",
    "W = np.random.randn(X.shape[0],1)\n",
    "b = 0\n",
    "\n",
    "print(\"m: \", m)\n",
    "print(\"W inicial: \",W)\n",
    "print(\"b inicial: \",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Apliquemos la regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Regresion logistica mediante descenso del gradiente\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(10000): #10000 iteraciones del descenso del gradiente\n",
    "    Z = None\n",
    "    A = None\n",
    "    dz = None\n",
    "    dW = None\n",
    "    db = None\n",
    "    J = None\n",
    "    \n",
    "    W -= None\n",
    "    b -= None\n",
    "    \n",
    "    if(i%1000 == 0):\n",
    "        print(\"costo: \", J)\n",
    "\n",
    "print(\"W actualizado: \",W)\n",
    "print(\"b actualizado: \",b)\n",
    "print(\"costo final (error), despues de \",i+1,\" iteraciones: \", J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(W,b,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_lr(W, b, X, y):\n",
    "    X = X.T\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    #Z = pred_func(W,b,np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = predict(W,b,np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_lr(W, b, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
