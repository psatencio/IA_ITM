{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"res/itm_logo.jpg\" width=\"300px\">\n",
    "\n",
    "## Inteligencia Artificial - IAI84\n",
    "### Instituto Tecnológico Metropolitano\n",
    "#### Pedro Atencio Ortiz - 2017\n",
    "\n",
    "\n",
    "En este notebook se abordan algunas prácticas para mejorar el desempeño de una red neuronal:\n",
    "1. Inicialización de parámetros\n",
    "2. Normalización de la entrada\n",
    "3. Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Caos 1: El problema XOR\n",
    "\n",
    "<img src='res/shallow_nn/xor_problem.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que ahora utilizamos una version traspuesta de los pesos de la red $W$, nuestra activation lineal cambia a $z = WX + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation(W, b, X):\n",
    "    z = np.dot(W, X) + b\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Returns sigmoid activation for array z\n",
    "    '''\n",
    "    a = 1. / (1. + np.exp(-z)) \n",
    "    \n",
    "    return a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivada de la funcion sigmoide, definida como $\\sigma'(z) = \\sigma(z)*(1-\\sigma(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1. - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(y, a):\n",
    "    return -(y * np.log(a) + (1.-y) * np.log(1-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(logloss):\n",
    "    return np.mean(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_multilayer(parameters,X):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "        \n",
    "    Z1 = linear_activation(W1,b1,X)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    Z2 = linear_activation(W2,b2,A1)\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    return np.round(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Trabajemos\n",
    "3. Realicemos descenso del gradiente sobre la red neural completa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Dataset XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_input(X):\n",
    "    v_x = np.var(X)\n",
    "    m_x = np.mean(X)\n",
    "    \n",
    "    X -= m_x\n",
    "    X /= v_x\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n",
    "Y = np.array([[0., 1., 1., 0.]])\n",
    "\n",
    "X = normalize_input(X)\n",
    "\n",
    "color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Inicializacion de parametros de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(neurons_hidden_layer):\n",
    "\n",
    "    len_X = 2\n",
    "    \n",
    "    W1 = np.random.randn(neurons_hidden_layer, len_X) * np.sqrt(2./float(len_X + neurons_hidden_layer))\n",
    "    b1 = np.zeros([neurons_hidden_layer,1])\n",
    "\n",
    "    W2 = np.random.randn(1,neurons_hidden_layer) * np.sqrt(2./float(neurons_hidden_layer+1))\n",
    "    b2 = np.zeros([1,1])\n",
    "    \n",
    "    parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### - Apliquemos descenso del gradiente a cada regresor logístico por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Metaparameters initialization\n",
    "'''\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "\n",
    "'''\n",
    "Parameters initialization\n",
    "'''\n",
    "parameters = initialize_parameters(3)\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]\n",
    "\n",
    "print (\"parametros iniciales: \", parameters)\n",
    "\n",
    "'''\n",
    "Gradient descent\n",
    "'''\n",
    "for i in range(num_epochs):\n",
    "    '''\n",
    "    Forward Propagation\n",
    "    '''\n",
    "    Z1 = linear_activation(W1, b1, X)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    Z2 = linear_activation(W2, b2, A1)\n",
    "    A2 = sigmoid(Z2)\n",
    "        \n",
    "    '''\n",
    "    Backward Propagation\n",
    "    '''\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T)\n",
    "    db2 = np.mean(np.sum(dZ2, axis=1, keepdims=True))\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), d_sigmoid(Z1))\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    db1 = np.mean(np.sum(dZ1, axis=1, keepdims=True))\n",
    "    \n",
    "    '''\n",
    "    Parameters Update\n",
    "    '''\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b1 -= learning_rate * db1\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    '''\n",
    "    Cost estimation\n",
    "    '''\n",
    "    J = cost(loss(Y,A2))\n",
    "    \n",
    "    \n",
    "    if(i%1000 == 0):\n",
    "        print(\"costo -- iteracion \", i, \": \", J)\n",
    "        \n",
    "print(\"parametros actualizados: \", parameters)\n",
    "\n",
    "'''\n",
    "Testing\n",
    "'''\n",
    "print(\"Predicciones del clasificador: \", predict_multilayer(parameters,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_lr(parameters, X, Y):\n",
    "    X = X.T\n",
    "    \n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    #Z = pred_func(W,b,np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = predict_multilayer(parameters, np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "    plt.scatter(X[:,0], X[:,1], color=color)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Visualizacion del resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_lr(parameters, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "# Caso 2: Dataset generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_1_layers(neurons_hidden_layer):\n",
    "\n",
    "    len_X = 2\n",
    "    \n",
    "    W1 = np.random.randn(neurons_hidden_layer, 2) / np.sqrt(2. / (len_X + neurons_hidden_layer))\n",
    "    b1 = np.zeros([neurons_hidden_layer,1])\n",
    "    \n",
    "    W2 = np.random.randn(1,neurons_hidden_layer) / np.sqrt(2. / (neurons_hidden_layer + 1) )\n",
    "    b2 = np.zeros([1,1])\n",
    "    \n",
    "    parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.exp(z)- np.exp(-z))/(np.exp(z)+ np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d_tanh(z):\n",
    "    return 1. - tanh(z)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_multilayer(parameters,X):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "        \n",
    "    Z1 = linear_activation(W1, b1, X)\n",
    "    A1 = tanh(Z1)\n",
    "    \n",
    "    Z2 = linear_activation(W2, b2, A1)\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    return np.round(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import generate_data\n",
    "\n",
    "X, Y = generate_data('blobs')\n",
    "X = normalize_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Metaparameters initialization\n",
    "'''\n",
    "num_epochs = 20000\n",
    "learning_rate = 0.001\n",
    "\n",
    "'''\n",
    "Parameters initialization\n",
    "'''\n",
    "parameters = initialize_parameters_1_layers(5)\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]\n",
    "\n",
    "print (\"parametros iniciales: \", parameters)\n",
    "\n",
    "'''\n",
    "Gradient descent\n",
    "'''\n",
    "for i in range(num_epochs):\n",
    "    '''\n",
    "    Forward Propagation\n",
    "    '''\n",
    "    Z1 = linear_activation(W1, b1, X)\n",
    "    A1 = tanh(Z1)\n",
    "    \n",
    "    Z2 = linear_activation(W2, b2, A1)\n",
    "    A2 = sigmoid(Z2)\n",
    "        \n",
    "    '''\n",
    "    Backward Propagation\n",
    "    '''\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T)\n",
    "    db2 = np.mean(np.sum(dZ2, axis=1, keepdims=True))\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), d_tanh(Z1))\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    db1 = np.mean(np.sum(dZ1, axis=1, keepdims=True))\n",
    "        \n",
    "    '''\n",
    "    Parameters Update\n",
    "    '''\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    '''\n",
    "    Cost estimation\n",
    "    '''\n",
    "    J = cost(loss(Y,A2))\n",
    "    \n",
    "    \n",
    "    if(i%1000 == 0):\n",
    "        print(\"costo -- iteracion \", i, \": \", J)\n",
    "        \n",
    "print(\"parametros actualizados: \", parameters)\n",
    "\n",
    "'''\n",
    "Testing\n",
    "'''\n",
    "print(\"Predicciones del clasificador: \", predict_multilayer(parameters,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_lr(parameters, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
